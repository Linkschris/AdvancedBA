{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import __version__\n",
    "tf.keras.__version__ = __version__\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Embedding, Reshape\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "import gym\n",
    "\n",
    "from taxi_env import TaxiPickupEnvSimplified, TaxiPickupEnvStandard, TaxiPickupEnvAdvanced\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Advanced environment (Deep Q-learning)\n",
    "\n",
    "In this part, we will consider a more complex environment where pickup requests can appear anywhere on our 5x5 grid world. When there are requests in a map cell, we will represent that with a red counter showing the number of requests in that cell.\n",
    "\n",
    "### Run Random Policy\n",
    "\n",
    "As we have been doing before, let's start by having a look at how that environment looks like using a random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0.]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = TaxiPickupEnvAdvanced()\n",
    "env.reset()\n",
    "# for _ in range(100):\n",
    "#     try:\n",
    "#         env.render()\n",
    "#         state = env.step(env.action_space.sample()) # take a random action\n",
    "#         time.sleep(1)\n",
    "#         clear_output(wait=True)\n",
    "#     except KeyboardInterrupt:\n",
    "#         break\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Discrete(5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space {}\".format(env.action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-learning (DQN)\n",
    "\n",
    "As you probably realized already, when we start considering more interesting and more realistic problems, the dimensionality of the state space can quickly become unmanegeable for standard tabular Q-learning approaches. In the environment from Part 2, with just 3 possible pickup locations, the number of possible (discrete) states was already $5\\times5\\times2^3 = 200$. This is not a problem just because of memory requirements, but the learning task of fitting Q-table becomes much more complex (observations becomes sparser - curse of dimensionality)! Not only that, but what happens when the states are continuous rather than discrete? We definitely need a way of tackling these problems...\n",
    "\n",
    "This is where function approximation comes in! In this tutorial, we will use neural networks as function approximators. This is also by far the most popular approach in the recent RL literature. To makes things easier, we will rely on 2 Python packages - keras and keras-rl - that abstract away many of the complexities involved in creating a neural network and using it for deep Q-learning.\n",
    "\n",
    "**Optional: Implementing neural networks in Keras** (if you want, you may skip this part and treat the neural network as black-box function approximator)\n",
    "\n",
    "Bulding a multi-layer neural network in Keras is fairly easy. We start by creating an object of the class \"Sequential\" (indicating that the neural network consists of sequence of layers):\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "```\n",
    "\n",
    "Now we can add layers to our neural network model. For examply, we can add a fully connected (dense) layer with 50 neurons and using a ReLU (rectified linear unit) activation function as follows:\n",
    "\n",
    "```python\n",
    "model.add(Dense(50, input_dim=30, activation='relu'))\n",
    "```\n",
    "\n",
    "A similar approach can be used for other types of layers such as Convolutional Layers. The following line adds a Convolutional layer with 20 filters of 3x3 convolutions:\n",
    "\n",
    "```python\n",
    "model.add(Conv2D(20, kernel_size=(3, 3), activation=\"relu\"))\n",
    "```\n",
    "We can now keep adding more hidden layers, or add the final Dense layer. Note that since this is a regression problem (i.e. we want the neural network to output Q-value for the different possible actions given the state passed as input to it), the last layer (output layer) of network must necessarily have as many neurons/outputs as there are actions in our RL problem and it must use a linear activation:\n",
    "\n",
    "```python\n",
    "model.add(Dense(env.nA, activation='linear'))\n",
    "```\n",
    "\n",
    "**MDP formulation**:\n",
    "\n",
    "Armed with the power of neural networks for function approximation, we can build a more complex representation of the environment state. In this case, we will represent the state using 4 5x5 matrices containing:\n",
    "\n",
    "- Matrix 1: position of the taxi, one-hot encoded (i.e. with \"1\" in the place where the taxi is located, and zeros else where). For example:\n",
    "\n",
    "``[[0,0,0,0,0],\n",
    " [0,0,0,0,0],\n",
    " [0,0,0,0,0],\n",
    " [0,0,0,1,0],\n",
    " [0,0,0,0,0]]``\n",
    "\n",
    "- Matrix 2: number of pickup requests in each cell. For example:\n",
    "\n",
    "``[[0,0,0,0,0],\n",
    " [0,0,0,1,0],\n",
    " [0,0,0,0,0],\n",
    " [2,0,0,0,4],\n",
    " [0,0,1,0,0]]``\n",
    " \n",
    "- Matrix 3: information about the presence of walls to the east. In this case:\n",
    "\n",
    "``[[0,1,0,0,0],\n",
    " [0,1,0,0,0],\n",
    " [0,0,0,0,0],\n",
    " [1,0,1,0,0],\n",
    " [1,0,1,0,0]]``\n",
    " \n",
    "- Matrix 4: information about the presence of walls to the west. In this case:\n",
    "\n",
    "``[[0,0,1,0,0],\n",
    " [0,0,1,0,0],\n",
    " [0,0,0,0,0],\n",
    " [0,1,0,1,0],\n",
    " [0,1,0,1,0]]``\n",
    " \n",
    "We can stack these 4 matrices together to create a 5x5x4 tensor that represents the state of the environment, and feed it to the neural network as input. The idea is for the neural network to learn to approximate the Q-values (i.e. expected future rewards) for the different possible actions given the state that was passed as input.\n",
    "\n",
    "Aside: we are not arguing that this is necessarily the best state representation for this problem. In fact, coming up with good state representation is a key challenge in deep RL and it can determine how fast your agent learns and how the policies that it learns can be. This state representation seems to perform acceptably well for this problem, so we will proceed with it.\n",
    "\n",
    "In summary, the new MDP for this revised version of the problem can be formalized as:\n",
    "\n",
    "**Actions:** north, south, east, west, pickup\n",
    "\n",
    "**State:** 5x5x4 tensor representing the position of the taxi, the number of requests in each cell and the locations of the \"walls\"\n",
    "\n",
    "**Reward:** +20 if taxi at a pickup location where there are requests and action is \"pickup\", else -1 (penalty for time elapsed); trying to pickup in a location different than the target also leads to penalty of -10.\n",
    "\n",
    "The code below creates a neural network in Keras that performs well in this environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_2 (Reshape)         (None, 5, 5, 4)           0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 20)          740       \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 180)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 5)                 905       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1645 (6.43 KB)\n",
      "Trainable params: 1645 (6.43 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Embedding, Reshape, Conv2D, MaxPooling2D, Input\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# First, we build a very simple neural network model in Keras\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(1, env.num_rows, env.num_columns, 4)))\n",
    "model.add(Reshape(target_shape=(env.num_rows, env.num_columns, 4)))\n",
    "model.add(Conv2D(20, kernel_size=(3, 3), activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(env.nA, activation='linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the neural network in place, it is time to use it as a function approximator for the Q-function of our RL agent. This can be easily done using the Keras-rl package in Python. The code below creates a deep Q-learning agent using $\\epsilon$-greedy exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from keras.src.saving import serialization_lib\n",
    "serialization_lib.enable_unsafe_deserialization()\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "# Then, define DQN agent in Keras-RL\n",
    "memory = SequentialMemory(limit=200000, window_length=1)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                              attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=100000)\n",
    "dqn = DQNAgent(model=model, nb_actions=env.nA, memory=memory, policy=policy, \n",
    "               nb_steps_warmup=500, target_model_update=1e-2, enable_double_dqn=True, enable_dueling_network=True)\n",
    "dqn.compile(optimizer=Adam(learning_rate=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence, we are doing the exact some thing as we did before: Q-learning with $\\epsilon$-greedy exploration. The key difference is in the way that we approximate the Q-function: before we used a table, and now we are using a neural network. \n",
    "\n",
    "As you can probably guess from the code above, we are also using a few popular RL techniques that improve the stability and convergence of Q-learning algorithms:\n",
    "\n",
    "- Experience replay: we add a ``memory`` that allows the RL agent to \"re-live\" past experience but accounting for the latest knowledge that it has about the Q-function.\n",
    "\n",
    "- Double deep Q networks (``enable_double_dqn=True``)\n",
    "\n",
    "- Dueling networks (``enable_dueling_network=True``)\n",
    "\n",
    "These fall outside of the scope of this tutorial, but they are explained in detail in the aditional materials provided in the last slides.\n",
    "\n",
    "We can now run our deep Q-learning algorithm (in this case for 400000 steps, where each episode has a maximum of 200 steps):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: -1.5229\n",
      "50 episodes - episode_reward: -304.580 [-545.000, -98.000] - loss: 38.575 - mae: 63.957 - mean_q: 82.891 - mean_eps: 0.953\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: -1.1071\n",
      "50 episodes - episode_reward: -221.420 [-386.000, -11.000] - loss: 89.134 - mae: 131.161 - mean_q: 167.712 - mean_eps: 0.865\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: -0.8044\n",
      "50 episodes - episode_reward: -160.880 [-389.000, 109.000] - loss: 128.200 - mae: 164.491 - mean_q: 209.714 - mean_eps: 0.775\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 789s 79ms/step - reward: -0.6514\n",
      "50 episodes - episode_reward: -130.280 [-341.000, 100.000] - loss: 143.402 - mae: 173.766 - mean_q: 221.247 - mean_eps: 0.685\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: -0.4567\n",
      "50 episodes - episode_reward: -91.340 [-326.000, 157.000] - loss: 139.060 - mae: 172.423 - mean_q: 219.538 - mean_eps: 0.595\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 69s 7ms/step - reward: -0.3313\n",
      "50 episodes - episode_reward: -66.260 [-350.000, 211.000] - loss: 135.366 - mae: 170.384 - mean_q: 216.896 - mean_eps: 0.505\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: -0.1495\n",
      "50 episodes - episode_reward: -29.900 [-377.000, 184.000] - loss: 127.079 - mae: 161.559 - mean_q: 205.813 - mean_eps: 0.415\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 119s 12ms/step - reward: 0.1241\n",
      "50 episodes - episode_reward: 24.820 [-248.000, 196.000] - loss: 120.011 - mae: 154.195 - mean_q: 196.599 - mean_eps: 0.325\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 126s 13ms/step - reward: 0.2636\n",
      "50 episodes - episode_reward: 52.720 [-221.000, 313.000] - loss: 115.335 - mae: 151.241 - mean_q: 192.855 - mean_eps: 0.235\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 194s 19ms/step - reward: 0.4277\n",
      "done, took 1636.406 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1c778691d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=100000, visualize=False, verbose=1, nb_max_episode_steps=200, log_interval=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the RL agent is learned, we can visualize the learned policy by exploiting the call-back functionality in Keras (don't worry if this code is a bit confusing for you as a first time Keras user; focus on the results that you obtained instead):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class Visualizer(Callback):\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    \n",
    "    def on_action_end(self, action, logs):\n",
    "        \"\"\" Render environment at the end of each action \"\"\"\n",
    "        self.env.render(mode='human')\n",
    "        time.sleep(1)\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| : | : : |\n",
      "| :\u001b[31m1\u001b[0m| :\u001b[31m1\u001b[0m: |\n",
      "| : : :\u001b[42m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[45m \u001b[0m| : | : |\n",
      "+---------+\n",
      "T: 70; Total earnings: 120\n",
      "Action: Pickup; Reward: 20\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dqn.test(env, nb_episodes=5, callbacks=[Visualizer(env)], nb_max_episode_steps=99, visualize=False, verbose=0)\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the results? Does the policy do what you would expect it to do? Or does it sometimes behaves strangely (i.e. not perfectly)? When I ran this code I managed to obtain a pretty good policy in a relatively short amount of time, but you can try doing a few tweaks to see if you improve. For example, try:\n",
    "\n",
    "- Increasing the number of training steps ``nb_steps=400000``\n",
    "\n",
    "- Increasing the maximum length of each episode ``nb_max_episode_steps=200``\n",
    "\n",
    "- Increasing the complexity of the neural network (e.g. more layers or more neurons per layer)\n",
    "\n",
    "- Changing the learning rate of the neural network optimizer ``Adam(lr=1e-3)``\n",
    "\n",
    "- Other hyper-parameters of the RL algorithm (e.g. memory size, decay rate of $\\epsilon$-greedy, etc.):\n",
    "\n",
    "``memory = SequentialMemory(limit=200000, window_length=1)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                              attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=100000)\n",
    "dqn = DQNAgent(model=model, nb_actions=env.nA, memory=memory, policy=policy, \n",
    "               nb_steps_warmup=500, target_model_update=1e-2, enable_double_dqn=True, enable_dueling_network=True)``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We hope that you enjoyed this brief introduction to the world of reinforcement learning :-)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
