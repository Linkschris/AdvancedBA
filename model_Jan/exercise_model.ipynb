{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import __version__\n",
    "tf.keras.__version__ = __version__\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Embedding, Reshape\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "import gym\n",
    "\n",
    "from env_class import BatteryManagementEnv\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50.], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = BatteryManagementEnv()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space 1\n",
      "State Space 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(env.nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "\n",
    "#build neural network for DQN\n",
    "def build_model(states, actions):\n",
    "    input = Input(shape=(1,states))\n",
    "    x = Flatten()(input)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    #output layer\n",
    "    output = Dense(actions, activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs=input, outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model(env.observation_space, env.nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from keras.src.saving import serialization_lib\n",
    "serialization_lib.enable_unsafe_deserialization()\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "# Then, define DQN agent in Keras-RL\n",
    "memory = SequentialMemory(limit=20000, window_length=1)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                              attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=10000)\n",
    "dqn = DQNAgent(model=model, nb_actions=env.nA, memory=memory, policy=policy,\n",
    "                nb_steps_warmup=500, target_model_update=1e-2, enable_double_dqn=True, enable_dueling_network=True)\n",
    "dqn.compile(optimizer=Adam(learning_rate=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "  17/1000 [..............................] - ETA: 3s - reward: -17.6471   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 113/1000 [==>...........................] - ETA: 2s - reward: -9.7345 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 14s 13ms/step - reward: 0.4000\n",
      "1000 episodes - episode_reward: 0.400 [-100.000, 100.000] - loss: 648.844 - mae: 50.016 - mean_q: 78.116 - mean_eps: 0.932\n",
      "\n",
      "Interval 2 (1000 steps performed)\n",
      "1000/1000 [==============================] - 20s 20ms/step - reward: 11.4000\n",
      "1000 episodes - episode_reward: 11.400 [-100.000, 100.000] - loss: 0.000 - mae: 50.000 - mean_q: 100.000 - mean_eps: 0.865\n",
      "\n",
      "Interval 3 (2000 steps performed)\n",
      "1000/1000 [==============================] - 19s 19ms/step - reward: 20.4000\n",
      "1000 episodes - episode_reward: 20.400 [-100.000, 100.000] - loss: 0.000 - mae: 50.000 - mean_q: 100.000 - mean_eps: 0.775\n",
      "\n",
      "Interval 4 (3000 steps performed)\n",
      "1000/1000 [==============================] - 21s 21ms/step - reward: 29.6000\n",
      "1000 episodes - episode_reward: 29.600 [-100.000, 100.000] - loss: 0.000 - mae: 50.000 - mean_q: 100.000 - mean_eps: 0.685\n",
      "\n",
      "Interval 5 (4000 steps performed)\n",
      "1000/1000 [==============================] - 21s 21ms/step - reward: 44.2000\n",
      "1000 episodes - episode_reward: 44.200 [-100.000, 100.000] - loss: 0.000 - mae: 50.000 - mean_q: 100.000 - mean_eps: 0.595\n",
      "\n",
      "Interval 6 (5000 steps performed)\n",
      "1000/1000 [==============================] - 22s 22ms/step - reward: 46.0000\n",
      "1000 episodes - episode_reward: 46.000 [-100.000, 100.000] - loss: 0.000 - mae: 50.000 - mean_q: 100.000 - mean_eps: 0.505\n",
      "\n",
      "Interval 7 (6000 steps performed)\n",
      "1000/1000 [==============================] - 22s 22ms/step - reward: 58.2000\n",
      "1000 episodes - episode_reward: 58.200 [-100.000, 100.000] - loss: 0.000 - mae: 50.000 - mean_q: 100.000 - mean_eps: 0.415\n",
      "\n",
      "Interval 8 (7000 steps performed)\n",
      "1000/1000 [==============================] - 23s 23ms/step - reward: 63.6000\n",
      "1000 episodes - episode_reward: 63.600 [-100.000, 100.000] - loss: 0.000 - mae: 50.000 - mean_q: 100.000 - mean_eps: 0.325\n",
      "\n",
      "Interval 9 (8000 steps performed)\n",
      "1000/1000 [==============================] - 21s 21ms/step - reward: 76.4000\n",
      "1000 episodes - episode_reward: 76.400 [-100.000, 100.000] - loss: 0.000 - mae: 50.000 - mean_q: 100.000 - mean_eps: 0.235\n",
      "\n",
      "Interval 10 (9000 steps performed)\n",
      "1000/1000 [==============================] - 23s 23ms/step - reward: 84.8000\n",
      "1000 episodes - episode_reward: 84.800 [-100.000, 100.000] - loss: 0.000 - mae: 50.000 - mean_q: 100.000 - mean_eps: 0.145\n",
      "\n",
      "Interval 11 (10000 steps performed)\n",
      "1000/1000 [==============================] - 17s 17ms/step - reward: 88.2000\n",
      "1000 episodes - episode_reward: 88.200 [-100.000, 100.000] - loss: 0.000 - mae: 50.000 - mean_q: 100.000 - mean_eps: 0.100\n",
      "\n",
      "Interval 12 (11000 steps performed)\n",
      "1000/1000 [==============================] - 16s 16ms/step - reward: 86.6000\n",
      "1000 episodes - episode_reward: 86.600 [-100.000, 100.000] - loss: 0.000 - mae: 50.000 - mean_q: 100.000 - mean_eps: 0.100\n",
      "\n",
      "Interval 13 (12000 steps performed)\n",
      "1000/1000 [==============================] - 17s 17ms/step - reward: 90.6000\n",
      "1000 episodes - episode_reward: 90.600 [-100.000, 100.000] - loss: 0.000 - mae: 50.000 - mean_q: 100.000 - mean_eps: 0.100\n",
      "\n",
      "Interval 14 (13000 steps performed)\n",
      "1000/1000 [==============================] - 17s 17ms/step - reward: 90.2000\n",
      "1000 episodes - episode_reward: 90.200 [-100.000, 100.000] - loss: 0.000 - mae: 50.000 - mean_q: 100.000 - mean_eps: 0.100\n",
      "\n",
      "Interval 15 (14000 steps performed)\n",
      " 228/1000 [=====>........................] - ETA: 14s - reward: 90.3509done, took 276.875 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x182301ae610>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=100000, visualize=False, verbose=1, nb_max_episode_steps=200, log_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
