{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = pd.read_csv('C:/Users/volco/AdvancedBA/Data/load.csv')\n",
    "prices = pd.read_csv('C:/Users/volco/AdvancedBA/Data/prices.csv')\n",
    "res_gen = pd.read_csv('C:/Users/volco/AdvancedBA/Data/res_gen.csv')\n",
    "residual_gen = pd.read_csv('C:/Users/volco/AdvancedBA/Data/residual_generation.csv')\n",
    "\n",
    "load['date'] = pd.to_datetime(load['date'])\n",
    "prices['date'] = pd.to_datetime(prices['date'])\n",
    "res_gen['date'] = pd.to_datetime(res_gen['date'])\n",
    "residual_gen['date'] = pd.to_datetime(residual_gen['date'])\n",
    "\n",
    "# Convert all price columns to numeric, coercing errors to NaN for non-numeric values in 'price_FRA'\n",
    "for col in ['price_AT', 'price_BE', 'price_FRA', 'price_GER', 'price_NL']:\n",
    "    prices[col] = pd.to_numeric(prices[col], errors='coerce')\n",
    "\n",
    "missing_values_prices = prices.isnull().sum()\n",
    "\n",
    "prices.fillna(prices.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the datasets\n",
    "load_data = pd.read_csv('C:/Users/volco/AdvancedBA/Data/load.csv', parse_dates=['date'], index_col='date')['load_GER']\n",
    "prices_data = pd.read_csv('C:/Users/volco/AdvancedBA/Data/prices.csv', parse_dates=['date'], index_col='date')['price_GER']\n",
    "res_gen_data = pd.read_csv('C:/Users/volco/AdvancedBA/Data/res_gen.csv', parse_dates=['date'], index_col='date')[['solar_forecastGER', 'windonshore_forecastGER']]\n",
    "\n",
    "# Combine into a single DataFrame\n",
    "data = pd.concat([load_data, prices_data, res_gen_data], axis=1).dropna()\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "data_scaled = pd.DataFrame(data_scaled, columns=data.columns, index=data.index)\n",
    "\n",
    "# Assuming we've preprocessed and scaled our data, let's move to defining the environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BatteryOptimizationEnv(gym.Env):\n",
    "#     def __init__(self, data):\n",
    "#         super(BatteryOptimizationEnv, self).__init__()\n",
    "#         self.data = data\n",
    "#         self.n_steps = data.shape[0]\n",
    "#         self.current_step = 0\n",
    "        \n",
    "#         self.action_space = spaces.Discrete(3)  # Actions: 0 = Do nothing, 1 = Charge, 2 = Discharge\n",
    "#         self.observation_space = spaces.Box(low=0, high=1, shape=(data.shape[1] + 1,), dtype=np.float32)  # State space + battery level\n",
    "        \n",
    "#         self.max_battery = 1.0  # Maximum normalized battery charge\n",
    "#         self.battery_charge = self.max_battery / 2  # Start with half charge\n",
    "#         self.charge_discharge_amount = 0.1  # Amount to charge or discharge at each action\n",
    "        \n",
    "#     def reset(self):\n",
    "#         self.current_step = 0\n",
    "#         self.battery_charge = self.max_battery / 2\n",
    "#         return self._next_observation()  # Should return ONLY the observation.\n",
    "    \n",
    "    \n",
    "#     def _next_observation(self):\n",
    "#         obs = self.data.iloc[self.current_step].values\n",
    "#         self.current_step += 1\n",
    "#         return np.append(obs, self.battery_charge)\n",
    "    \n",
    "#     def step(self, action):\n",
    "#         reward = 0\n",
    "#         done = False\n",
    "        \n",
    "#         # Implement the logic for charging, discharging, or doing nothing\n",
    "#         # Update the battery_charge and calculate the reward based on the action\n",
    "#         # Simplified example:\n",
    "#         if action == 1:\n",
    "#             self.battery_charge = min(self.max_battery, self.battery_charge + self.charge_discharge_amount)\n",
    "#             reward = -1  # Simplified reward, consider energy costs and savings\n",
    "#         elif action == 2:\n",
    "#             self.battery_charge = max(0, self.battery_charge - self.charge_discharge_amount)\n",
    "#             reward = 1  # Simplified reward\n",
    "        \n",
    "#         if self.current_step >= self.n_steps:\n",
    "#             done = True\n",
    "        \n",
    "#         return self._next_observation(), reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Ensure you have the correct version of gym installed\n",
    "# # If not, you might need to install it via pip\n",
    "# # e.g., !pip install gym\n",
    "\n",
    "# # Load and preprocess your data here\n",
    "# # Placeholder for data loading and preprocessing\n",
    "\n",
    "# # Defining the Battery Optimization Environment\n",
    "# class BatteryOptimizationEnv(gym.Env):\n",
    "#     \"\"\"A battery charge/discharge environment for OpenAI gym\"\"\"\n",
    "#     metadata = {'render.modes': ['human']}\n",
    "\n",
    "#     def __init__(self, data):\n",
    "#         super(BatteryOptimizationEnv, self).__init__()\n",
    "#         self.data = data\n",
    "#         self.current_step = 0\n",
    "#         self.done = False\n",
    "#         self.max_battery = 100  # Maximum battery capacity in kWh\n",
    "#         self.battery_level = self.max_battery / 2  # Starting battery level\n",
    "#         self.charge_rate = 10  # kWh\n",
    "\n",
    "#         # Define action and observation space\n",
    "#         # Actions: 0 = Do Nothing, 1 = Charge, 2 = Discharge\n",
    "#         self.action_space = spaces.Discrete(3)\n",
    "\n",
    "#         # Observation space: normalized data features + battery level as a percentage of max capacity\n",
    "#         self.observation_space = spaces.Box(low=0, high=1,\n",
    "#                                             shape=(data.shape[1] + 1,), dtype=np.float32)\n",
    "\n",
    "#     def _next_observation(self):\n",
    "#         obs = np.append(self.data.iloc[self.current_step].values, self.battery_level / self.max_battery)\n",
    "#         return obs\n",
    "\n",
    "#     def step(self, action):\n",
    "#         # Implement the logic to update the battery level based on the action\n",
    "#         # Simplified reward logic based on action choice\n",
    "#         reward = 0\n",
    "#         if action == 1:  # Charging\n",
    "#             self.battery_level = min(self.battery_level + self.charge_rate, self.max_battery)\n",
    "#             reward = -1  # Example penalty for charging\n",
    "#         elif action == 2:  # Discharging\n",
    "#             self.battery_level = max(self.battery_level - self.charge_rate, 0)\n",
    "#             reward = 1  # Example reward for discharging\n",
    "\n",
    "#         self.current_step += 1\n",
    "\n",
    "#         if self.current_step >= len(self.data):\n",
    "#             self.done = True\n",
    "\n",
    "#         return self._next_observation(), reward, self.done, {}\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.current_step = 0\n",
    "#         self.battery_level = self.max_battery / 2\n",
    "#         self.done = False\n",
    "#         return self._next_observation()\n",
    "\n",
    "#     def render(self, mode='human', close=False):\n",
    "#         # This method can be used for rendering the environment's state if necessary\n",
    "#         print(f'Step: {self.current_step}, Battery Level: {self.battery_level} kWh')\n",
    "\n",
    "# env = BatteryOptimizationEnv(data_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        self.action_space = spaces.Discrete(2)  # Example: 2 discrete actions\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)  # Example observation space\n",
    "\n",
    "    def step(self, action):\n",
    "        observation = np.array([0.0], dtype=np.float32)  # Placeholder observation, ensure dtype matches observation_space\n",
    "        reward = 0  # Placeholder reward\n",
    "        done = False  # Indicates if episode has ended\n",
    "        truncated = False  # Indicates if episode was truncated (e.g., due to time limit)\n",
    "        info = {}  # Additional info, can be empty\n",
    "        return observation, reward, done, truncated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)  # Optional: reset the environment's random state\n",
    "        observation = np.array([0.0], dtype=np.float32)  # Reset observation, ensure dtype matches observation_space\n",
    "        return observation, {}  # Reset returns observation and optionally an info dictionary\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        pass  # Implement rendering if necessary for your environment\n",
    "\n",
    "# Assuming stable_baselines3 and its env checker are compatible with gymnasium\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = CustomEnv()\n",
    "check_env(env)  # Should now pass without assertion errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1486 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 1017     |\n",
      "|    iterations           | 2        |\n",
      "|    time_elapsed         | 4        |\n",
      "|    total_timesteps      | 4096     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.693   |\n",
      "|    explained_variance   | nan      |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 0        |\n",
      "|    n_updates            | 10       |\n",
      "|    policy_gradient_loss | 0        |\n",
      "|    value_loss           | 0        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 887      |\n",
      "|    iterations           | 3        |\n",
      "|    time_elapsed         | 6        |\n",
      "|    total_timesteps      | 6144     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.693   |\n",
      "|    explained_variance   | nan      |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 0        |\n",
      "|    n_updates            | 20       |\n",
      "|    policy_gradient_loss | 0        |\n",
      "|    value_loss           | 0        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 846      |\n",
      "|    iterations           | 4        |\n",
      "|    time_elapsed         | 9        |\n",
      "|    total_timesteps      | 8192     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.693   |\n",
      "|    explained_variance   | nan      |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 0        |\n",
      "|    n_updates            | 30       |\n",
      "|    policy_gradient_loss | 0        |\n",
      "|    value_loss           | 0        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 825      |\n",
      "|    iterations           | 5        |\n",
      "|    time_elapsed         | 12       |\n",
      "|    total_timesteps      | 10240    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.693   |\n",
      "|    explained_variance   | nan      |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 0        |\n",
      "|    n_updates            | 40       |\n",
      "|    policy_gradient_loss | 0        |\n",
      "|    value_loss           | 0        |\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "\n",
    "# To load the model\n",
    "model = PPO.load(\"ppo_battery_optimization\")\n",
    "obs, _info = env.reset()  # Unpack the tuple to get the observation\n",
    "for _ in range(1000):  # Adjust the number of steps as necessary\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, done, truncated, info = env.step(action)  # Ensure env.step() also returns the correct number of values\n",
    "    env.render()  # Optionally render the environment's state if applicable\n",
    "    if done:\n",
    "        obs, _info = env.reset() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Number of episodes for evaluation\n",
    "# num_episodes = 100\n",
    "\n",
    "# episode_rewards = []\n",
    "# chosen_actions = []\n",
    "\n",
    "# for episode in range(num_episodes):\n",
    "#     obs, _info = env.reset()\n",
    "#     done = False\n",
    "#     episode_reward = 0\n",
    "#     episode_actions = []\n",
    "    \n",
    "#     while not done:\n",
    "#         action, _states = model.predict(obs, deterministic=True)\n",
    "#         obs, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "#         episode_reward += reward\n",
    "#         episode_actions.append(action)\n",
    "    \n",
    "#     episode_rewards.append(episode_reward)\n",
    "#     chosen_actions.append(episode_actions)\n",
    "\n",
    "# # Calculate and print the average reward per episode\n",
    "# average_reward = sum(episode_rewards) / num_episodes\n",
    "# print(f'Average Reward per Episode: {average_reward}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(env, model, num_episodes=1):\n",
    "#     episode_rewards = []\n",
    "#     for episode in range(num_episodes):\n",
    "#         obs, _ = env.reset()\n",
    "#         done = False\n",
    "#         total_rewards = 0\n",
    "#         while not done:\n",
    "#             action, _states = model.predict(obs, deterministic=True)\n",
    "#             obs, rewards, done, truncated, info = env.step(action)\n",
    "#             total_rewards += rewards\n",
    "#             if done:\n",
    "#                 break\n",
    "#         episode_rewards.append(total_rewards)\n",
    "#     average_reward = sum(episode_rewards) / num_episodes\n",
    "#     print(f\"Average reward over {num_episodes} episodes: {average_reward}\")\n",
    "#     return average_reward\n",
    "\n",
    "# # Evaluate the model\n",
    "# average_reward = evaluate_model(env, model, num_episodes=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize the distribution of episode rewards\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(episode_rewards, bins=20)\n",
    "# plt.title('Distribution of Episode Rewards')\n",
    "# plt.xlabel('Total Reward')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "# # Analyze the distribution of actions\n",
    "# # Assuming actions: 0 = Do Nothing, 1 = Charge, 2 = Discharge\n",
    "# action_labels = ['Do Nothing', 'Charge', 'Discharge']\n",
    "# action_counts = [sum(actions.count(action) for actions in chosen_actions) for action in range(len(action_labels))]\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.bar(action_labels, action_counts)\n",
    "# plt.title('Distribution of Actions Taken')\n",
    "# plt.xlabel('Action')\n",
    "# plt.ylabel('Count')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    \"\"\"Custom environment for battery optimization\"\"\"\n",
    "    def __init__(self):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        self.action_space = spaces.Discrete(3)  # Actions: Do nothing, Charge, Discharge\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(5,), dtype=np.float32)\n",
    "        # Initialize additional attributes here (e.g., battery level)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Implement action logic, state transition, and reward calculation\n",
    "        observation = np.array([0.0], dtype=np.float32)\n",
    "        reward = 0\n",
    "        done = False\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        return observation, reward, done, truncated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # Reset the environment state\n",
    "        observation = np.array([0.0], dtype=np.float32)\n",
    "        return observation, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs, _info = env.reset()  # Reset the environment to get the initial observation\n",
    "\n",
    "# # Ensure obs is correctly shaped. This line might be redundant if obs is already in the correct shape.\n",
    "# # It's provided here for demonstration purposes.\n",
    "# obs = obs.reshape((5,))  # Adjust this reshape operation based on your actual observation space's expected shape\n",
    "\n",
    "# for _ in range(1000):  # Adjust the number of steps as necessary\n",
    "#     action, _states = model.predict(obs, deterministic=True)\n",
    "    \n",
    "#     obs, rewards, done, truncated, info = env.step(action)\n",
    "#     # After env.step(), ensure obs is correctly shaped before the next predict() call\n",
    "#     obs = obs.reshape((5,))  # Ensure this matches your observation space's shape\n",
    "    \n",
    "#     env.render()  # Optionally render the environment's state if applicable\n",
    "    \n",
    "#     if done:\n",
    "#         obs, _info = env.reset()  # Reset the environment if done\n",
    "#         obs = obs.reshape((5,))  # Ensure obs is correctly shaped after reset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
